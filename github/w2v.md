# Word2Vector 模型

下面提供一些训练好的 Word2Vector 模型，供大家使用。

### 202103

使用 2021 年 3 月的 Wiki 中文语料训练，使用 Skip-Gram 模型训练，单词维数 300，训练窗口 10。

- 【模型】[Google_word2vec_zhwiki2103_300d.bin](https://pan.baidu.com/s/1Wtzz0mzhsaAcw6WVOvhgLQ) (提取码：izv2)
- 【语料】[zhwiki_2103_preprocessed.simplied.zip](https://pan.baidu.com/s/1wLlQ8Z5fH2fCkvFJowWQLg) (提取码：eq2k)

注：语料已经使用 [AHANLP](https://github.com/jsksxs360/AHANLP) 做过分词处理（未去停用词）。

### 201710

使用 2017 年 10 月的 Wiki 中文语料训练，使用 Skip-Gram 模型训练，单词维数 300，训练窗口 10。

- 【模型】[Google_word2vec_zhwiki1710_300d.bin](http://pan.baidu.com/s/1i4BLryH)
- 【语料】[zhwiki_1710_preprocessed.simplied.zip](http://pan.baidu.com/s/1hsMmRbu)

注：语料已经使用 [AHANLP](https://github.com/jsksxs360/AHANLP) 做过分词处理（未去停用词）。

### 201709

使用 2017 年 9 月的 Wiki 中文语料训练，使用 Skip-Gram 模型训练，单词维数 300，训练窗口 5。

- 【模型】[Google_word2vec_zhwiki1709_300d.bin](https://pan.baidu.com/s/1o8zEuYA)
- 【语料】[zhwiki_1709_preprocessed.simplied.zip](https://pan.baidu.com/s/1nuXWdUL)

注：语料已经使用 [AHANLP](https://github.com/jsksxs360/AHANLP) 做过分词和去停用词处理。

### 旧版

单词维数 200。

- 【模型】[wiki_chinese_word2vec(Google).model](https://pan.baidu.com/s/1kUD0jzh)
- 【语料】[wiki_chinese_preprocessed.simplied.txt.tar.gz](https://pan.baidu.com/s/1dFgIbTZ)

注：语料已做过预处理。
